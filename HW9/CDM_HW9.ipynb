{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NchqVoT3OL8f"
      },
      "source": [
        "# Extracting Keywords and Important Sentences from a Text\n",
        "\n",
        "---\n",
        "#### Course: Computational Data Mining\n",
        "#### Professor: Dr. Fatemeh Shakeri\n",
        "#### Student: Ilya Khalafi\n",
        "#### Student ID: 9913039\n",
        "#### January 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuZQ4wht7zlb"
      },
      "source": [
        "# Table Of Contents\n",
        "- [Introduction](#intro)\n",
        "- [Dependencies](#dependency)\n",
        "- [Preparing the Data](#data)\n",
        "  - [Importing the Text](#import)\n",
        "  - [Text Preprocessing](#preprocessing)\n",
        "  - [Extracting Sentences & Their Words](#extract)\n",
        "- [Implementations](#implement)\n",
        "  - [Method 1](#impmethod1)\n",
        "  - [Method 2](#impmethod2)\n",
        "- [Applying Implemented methods](#apply)\n",
        "- [Final Conclusion](#conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpkdYRqO5VEj"
      },
      "source": [
        "<a name=\"intro\"></a>\n",
        "\n",
        "# Introduction üìö\n",
        "\n",
        "---\n",
        "\n",
        "In natural language processing, we aim to design mathematical methods to comprehend texts automatically and help us to accomplish several tasks such as classification, summarization, etc. One important goal is to automatically detect and extract keywords and significant sentences.\n",
        "\n",
        "<img src=\"https://previews.123rf.com/images/lculig/lculig1409/lculig140900234/31533068-keyword-concept-word-cloud-background.jpg\" width=\"400\"/>\n",
        "\n",
        "In this article, we implement and apply two computational methods to automatically extract important words and sentences.\n",
        "\n",
        " 1. In the first method, we define a custom score function for each pair of words and sentences and then define a matrix containing these scores. finally, we apply SVD to this matrix to extract important features.\n",
        "   \n",
        " 2. Unfortunately, the first method does not cross out similar sentences when it reports a sentence as important, so it may report several similar sentences as important. In the second method, we use householder matrices to decrease the score of sentences that are similar to the reported sentences; therefore, our method only reports one of similar sentences as important and crosses out other sentences that are similar to the reported one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXR_nHyMBCVI"
      },
      "source": [
        "<a name=\"dependency\"></a>\n",
        "\n",
        "# Dependencies üß∞\n",
        "\n",
        "---\n",
        "\n",
        "We need the following libraries during this article:\n",
        "\n",
        "- **numpy** : <br />\n",
        "    numpy is a commonly used library for doing scientific computation. Unlike Python's default pointer structure, numpy saves variables in place and continuously on RAM and also provides sophisticated methods that use parallelism to make our computations much faster.\n",
        "\n",
        "- **Matplotlib**: <br />\n",
        "    Matplotlib is a super useful Python library for drawing charts and data visualization. We use it to plot our facial images.\n",
        "\n",
        "- **seaborn** : <br />\n",
        "    seaborn is built on Matplotlib and provides many chart templates for us, so we don't need to draw and build every component of our charts with Matplotlib. We will use it to plot the final confusion matrix.\n",
        "\n",
        "- **scikit-learn (sklearn)** : <br />\n",
        "    Sklearn is a well-known library that has an implementation of machine learning models for several tasks. Here, we use the NMF class for the implementation of the second method.\n",
        "\n",
        "- **nltk** :\n",
        "    nltk stands for Natural Language Toolkit and contains several efficient implementations of methods for nlp. We will use it to remove stop words and replace words with their stem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sd9VGeCJBDXg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Python Standard Libraries\n",
        "import re\n",
        "# Fundamental Data Analysis & Visualization Tools\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import NMF\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dufiEBk54QdX"
      },
      "source": [
        "<a name=\"data\"></a>\n",
        "\n",
        "# Preparing the Data \n",
        "\n",
        "---\n",
        "\n",
        "In this section, we download our dataset from my Google Drive and then import its content. Then we remove its stop words and replace each word with its stem, and finally, we extract sentences from the text and convert our text into a list of lists where each list represents a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Q7CYvdAioI"
      },
      "source": [
        "<a name=\"import\"></a>\n",
        "\n",
        "### Importing the Text üîΩ\n",
        "\n",
        "---\n",
        "\n",
        "Let's download our data. It is simply a text file that contains text of 10 pages (258 ~ 268) of the book \"The Design of Everyday Things\". We use gdrive command to download this file from my Google Drive... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dUdLIRQM4QdX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Downloading the text file\n",
        "!gdown '1y08C3tczKpmexAJZDgrCOOmXlPOBQ8tW'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's read it as a string and print some of its content..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The realities of the world impose severe constraints upon the design of products\n"
          ]
        }
      ],
      "source": [
        "# Reading the text from the txt file\n",
        "data = open('book.txt', 'r').readlines()[0].strip()\n",
        "# Printing first 80 characters of the text\n",
        "print(data[:80])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! Let's proceed to the next section and clean this text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb46K5dwAioK"
      },
      "source": [
        "<a name=\"preprocessing\"></a>\n",
        "\n",
        "### Text Preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "Before processing the data and extracting keywords, we have to preprocess our text. In this section, we apply these three methods to our text:\n",
        "\n",
        "1. While extracting keywords, we need to omit to stop words such as 'a', 'an', the,' etc. Because these words commonly occur in any text and may confuse our model to report them as significant words. \n",
        "\n",
        "2. Additionally, we need to remove unnecessary characters such as ',', ':', ';' and parenthesis, and replace '!' and '?' with a simple period.\n",
        "\n",
        "3. A single word can have several forms, whether it is a noun, adjective, adverb, etc. This is an issue because it makes our feature matrix much larger and more sparse. Therefore, we replace each word with its stem to address this problem, but "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZEVJd46OAioK",
        "outputId": "a567b351-8cc8-4d75-f703-a8459997dfff"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords_and_signs(text):\n",
        "    # Downloading the required NLTK resources\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    \n",
        "    # Removing stop words and unnecessary characters\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = re.sub(r'[,:;()\\‚Äú\\‚Äù\\\"\\']', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "\n",
        "    # Replacing \"!\" and \"?\" with a single period\n",
        "    return re.sub(r'[!?]+', '.', text)\n",
        "\n",
        "def stem_words(text):\n",
        "    # Replacing each word with its stem\n",
        "    stemmer = PorterStemmer()\n",
        "    return ' '.join([stemmer.stem(word) for word in text.split()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's apply this method to apply all three mentioned changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 3 sentences of the crude data:\n",
            " The realities of the world impose severe constraints upon the design of products\n",
            " Up to now I have described the ideal case, assuming that human-centered design principles could be followed in a vacuum; that is, without attention to the real world of competition, costs, and schedules\n",
            " Conflicting requirements will come from different sources, all of which are legitimate, all of which need to be resolved\n",
            "------------------------------\n",
            "First 3 sentences of the preprocessed data:\n",
            " realiti world impos sever constraint upon design products\n",
            " describ ideal case assum human-cent design principl could follow vacuum without attent real world competit cost schedules\n",
            " conflict requir come differ sourc legitim need resolved\n"
          ]
        }
      ],
      "source": [
        "# Removing stop words and unnecessary signs\n",
        "cleaned_text = remove_stopwords_and_signs(data)\n",
        "\n",
        "# We have to keep a list of original sentences to reconstruct sentences later\n",
        "original_sentences = [sentence.strip() for sentence in cleaned_text.split('.') if len(sentence.strip()) != 0]\n",
        "\n",
        "# Replacing words with their stems\n",
        "text = stem_words(cleaned_text)\n",
        "\n",
        "# Comparing first three sentences of the preprocessed string with the crude data\n",
        "print('First 3 sentences of the crude data:\\n', '\\n'.join(data.split('.')[0:3]))\n",
        "print('-' * 30)\n",
        "print('First 3 sentences of the preprocessed data:\\n', '\\n'.join(text.split('.')[0:3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome! Notice that all mentioned changes are applied to the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"extract\"></a>\n",
        "\n",
        "### Extracting Sentences & Their Words\n",
        "\n",
        "---\n",
        "\n",
        "Before proceeding to the next section, we have to convert our text into a list of lists where each list contains a token of the words that are present in it. Therefore, we extract unique words from our text and assign a token to it (its index in the list of unique words). Then we convert our text into a list of lists of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of unique words:  803\n",
            "Total number of sentences:  182\n"
          ]
        }
      ],
      "source": [
        "# Defining a set that will contain all words present in the preprocessed text\n",
        "words = set()\n",
        "for sentence in text.split('.'):\n",
        "    words = words.union(set(sentence.strip().split()))\n",
        "words = list(words)\n",
        "    \n",
        "# Converting a set of words to a dictionary of words and their tokens\n",
        "word_to_token = {word:token for token, word in enumerate(words)}\n",
        "token_to_word = {token:word for word, token in word_to_token.items()}\n",
        "\n",
        "# Making a list of lists where each list represents a sentence and contains a list of strings\n",
        "tokenized_sentences = [[word_to_token[word.strip()] for word in sentence.split()] for sentence in text.split('.') if len(sentence.strip()) != 0]\n",
        "\n",
        "# Printing statistics of words and sentences\n",
        "print('Total number of unique words: ', len(words))\n",
        "print('Total number of sentences: ', len(tokenized_sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perfect! Now let's proceed to the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxr3SFz44Ngl"
      },
      "source": [
        "<a name=\"implement\"></a>\n",
        "\n",
        "# Implementations üß®\n",
        "\n",
        "---\n",
        "\n",
        "Now let's implement the methods that we mentioned earlier!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIoz9Fzv6T7D"
      },
      "source": [
        "<a name=\"impmethod1\"></a>\n",
        "\n",
        "### Method 1\n",
        "\n",
        "---\n",
        "\n",
        "The naive approach involves creating a unique scoring function for every combination of words and sentences. These scores are then assembled into a matrix, which is subsequently subjected to singular value decomposition (SVD) to extract significant characteristics.\n",
        "\n",
        "We may consider the score of each word-sentence as the number of occurrences of the word in the sentence. Additionally, we can consider the following score to define our matrix of score:\n",
        "\n",
        "> a[i][j] = f[i][j] * log(n / n_i)\n",
        "\n",
        "where:\n",
        "1. f[i][j] = Number of occurrences of the word i in sentence j\n",
        "2. n = Total number of sentences\n",
        "3. n_i = Total number of sentences containing word i\n",
        "\n",
        "We implement both options for our word-sentence matrix...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "19ehOcmU4Qde"
      },
      "outputs": [],
      "source": [
        "def naive_analysis(sentences, words, matrix_mode='score'):\n",
        "    '''\n",
        "        args:\n",
        "        1. sentences = a list of lists which each list contains the token of words\n",
        "        2. words = list of unique words of the text\n",
        "        3. matrix_mode = 'score', 'frequency'\n",
        "        output:\n",
        "        returns two lists, containing sorted rankings of words and sentences\n",
        "    '''\n",
        "    freq_mat = np.zeros((len(words), len(sentences)))\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for token in sentence:\n",
        "            freq_mat[token, i] += 1\n",
        "            \n",
        "    if matrix_mode == 'score':\n",
        "        n_i = (freq_mat > 0).astype(int).sum(axis=1)\n",
        "        freq_mat = freq_mat * np.log(len(sentences) / n_i)[:, np.newaxis]\n",
        "    \n",
        "    U, _, V = np.linalg.svd(freq_mat)\n",
        "    word_ranks = np.abs(U[:, 0]).argsort()[::-1]\n",
        "    sentence_ranks = np.abs(V[0]).argsort()[::-1]\n",
        "    return word_ranks, sentence_ranks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWHN4Q17KAcO"
      },
      "source": [
        "<a name=\"impmethod2\"></a>\n",
        "\n",
        "### Method 2\n",
        "\n",
        "---\n",
        "\n",
        "Regrettably, the first approach fails to eliminate comparable sentences when identifying an important sentence, potentially labeling multiple similar sentences as significant. In the second method, we employ Householder matrices to reduce the scores of sentences that resemble the identified sentences. Consequently, our method only designates one similar sentence as important while disregarding other sentences resembling the selected one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zw1eOnQQ4Qdg"
      },
      "outputs": [],
      "source": [
        "def uniqueness_analysis(sentences, words, n_components=10):\n",
        "    '''\n",
        "        args:\n",
        "        1. sentences = a list of lists which each list contains the token of words\n",
        "        2. words = list of unique words of the text\n",
        "        3. n_components = total number of top-ranked sentences to find\n",
        "        output:\n",
        "        returns one list, containing sorted rankings of sentences\n",
        "    '''\n",
        "    A = np.zeros((len(words), len(sentences)))\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for token in sentence:\n",
        "            A[token, i] += 1\n",
        "    \n",
        "    nmf = NMF(n_components=n_components, init='random', random_state=41)\n",
        "    W = nmf.fit_transform(A)\n",
        "    H = nmf.components_\n",
        "    \n",
        "    swaps = []\n",
        "    for col_idx in range(min(H.shape)):\n",
        "        max_norm_idx = np.argmax(np.linalg.norm(H[col_idx:, col_idx:], axis=0))\n",
        "        swaps.append((col_idx, max_norm_idx))\n",
        "        A[:, [col_idx, max_norm_idx]] = A[:, [max_norm_idx, col_idx]]  # Swap columns for permutation\n",
        "        \n",
        "        x = H[col_idx:, col_idx]\n",
        "        e = np.zeros_like(x)\n",
        "        e[0] = np.linalg.norm(x)\n",
        "        u = x - e\n",
        "        v = u / (np.linalg.norm(u) + 1e-16)\n",
        "        # Construct Householder matrix\n",
        "        Q = np.eye(H.shape[0])\n",
        "        Q[col_idx:, col_idx:] -= 2.0 * np.outer(v, v)\n",
        "        \n",
        "        W = W @ Q\n",
        "        H = Q.T @ H\n",
        "        k = H[col_idx, col_idx]\n",
        "        H[col_idx] /= k\n",
        "        W[:, col_idx] *= k\n",
        "        \n",
        "    indices = list(range(A.shape[1]))\n",
        "    for swap in swaps:\n",
        "        indices[swap[0]], indices[swap[1]] = indices[swap[1]], indices[swap[0]]\n",
        "    return indices[:n_components]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXhtXIJQAiob"
      },
      "source": [
        "<a name=\"apply\"></a>\n",
        "\n",
        "# Applying Implemented methods\n",
        "\n",
        "---\n",
        "\n",
        "Finally, we apply the implemented methods and print the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5dkrR0Q4Aiob"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 sentences from method-1 with score matrix: \n",
            "1.  pressures larger screens forced demise physical keyboards despite attempt make tiny keyboards operated single fingers thumbs keyboards displayed screen whenever needed letter tapped one time\n",
            "2.  anyone type dictate take photographs videos draw animated scenes creatively produce experiences twentieth century required huge amounts technology large crews specialized workers types devices allow us tasks ways controlled proliferate\n",
            "3.  Reading done quickly possible read around three hundred words per minute skim jumping ahead back effectively acquiring information rates thousands words per minute\n",
            "4.  Today talking video conferences writing photography still video collaborative interaction sorts increasingly done one single device available large variety screen sizes computational power portability\n",
            "5.  written using traditional keyboards even new technological devices keyboard still remains fastest way enter words system whether paper electronic physical virtual\n",
            "--------------------------------------------------\n",
            "Top 5 words from method-1 with score matrix: \n",
            "1.  keyboard\n",
            "2.  word\n",
            "3.  product\n",
            "4.  even\n",
            "5.  new\n",
            "--------------------------------------------------\n",
            "Top 5 sentences from method-1 with frequency matrix: \n",
            "1.  example illustrates real business pressures companies need speed concern costs competition may force company change offerings need satisfy several classes customers‚Äîinvestors distributors course people actually use product\n",
            "2.  competing company adds new features products producing competitive pressures match offering even order get ahead competition\n",
            "3.  Even first versions product well done human-centered focused upon real needs rare organization content let good product stay untouched\n",
            "4.  commission first production hand-tooled working prototypes could shown potential investors customers expensive proposition small self-funded company companies started displaying similar concepts trade shows\n",
            "5.  products two companies match feature feature longer reason customer prefer one another\n",
            "--------------------------------------------------\n",
            "Top 5 words from method-1 with frequency matrix: \n",
            "1.  product\n",
            "2.  new\n",
            "3.  featur\n",
            "4.  compani\n",
            "5.  competit\n",
            "--------------------------------------------------\n",
            "Top 5 sentences from method-2 with frequency matrix: \n",
            "1.  anyone type dictate take photographs videos draw animated scenes creatively produce experiences twentieth century required huge amounts technology large crews specialized workers types devices allow us tasks ways controlled proliferate\n",
            "2.  new media new technologies supplement old writing longer dominate much past medium widely available\n",
            "3.  Listening slow serial usually around sixty words per minute although rate doubled tripled speech compression technologies training still slower reading easy skim\n",
            "4.  Let‚Äôs call smart screens\n",
            "5.  doesn‚Äôt make sense call computers phones cameras need new name\n"
          ]
        }
      ],
      "source": [
        "# Feel free to change k to print k-top sentences\n",
        "k = 5\n",
        "\n",
        "# Method 1 with Score Matrix - Results\n",
        "word_ranks, sentence_ranks = naive_analysis(tokenized_sentences, words, matrix_mode='score')\n",
        "print(f'Top {k} sentences from method-1 with score matrix: ')\n",
        "for i, idx in enumerate(sentence_ranks[:k]):\n",
        "    print(f'{i+1}. ', original_sentences[idx])\n",
        "\n",
        "print('-' * 50)\n",
        "print(f'Top {k} words from method-1 with score matrix: ')\n",
        "for i, idx in enumerate(word_ranks[:k]):\n",
        "    print(f'{i+1}. ', words[idx])\n",
        "\n",
        "# Method 1 with Frequency Matrix - Results\n",
        "print('-' * 50)\n",
        "word_ranks, sentence_ranks = naive_analysis(tokenized_sentences, words, matrix_mode='frequency')\n",
        "print(f'Top {k} sentences from method-1 with frequency matrix: ')\n",
        "for i, idx in enumerate(sentence_ranks[:k]):\n",
        "    print(f'{i+1}. ', original_sentences[idx])\n",
        "    \n",
        "print('-' * 50)\n",
        "print(f'Top {k} words from method-1 with frequency matrix: ')\n",
        "for i, idx in enumerate(word_ranks[:k]):\n",
        "    print(f'{i+1}. ', words[idx])\n",
        "\n",
        "# Method 2\n",
        "print('-' * 50)\n",
        "sentence_ranks = uniqueness_analysis(tokenized_sentences, words, n_components=k)\n",
        "print(f'Top {k} sentences from method-2 with frequency matrix: ')\n",
        "for i, idx in enumerate(sentence_ranks):\n",
        "    print(f'{i+1}. ', original_sentences[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZlDKSseAiob"
      },
      "source": [
        "<a name=\"conclusion\"></a>\n",
        "\n",
        "# Final Conclusion\n",
        "\n",
        "---\n",
        "\n",
        "Here is our observation for each method...\n",
        "\n",
        "Method-1:\n",
        "\n",
        "- Sentences: The sentences selected by this method focus on various topics such as the evolution of keyboards, technological advancements, reading speed, and the importance of keyboards in entering words into a system. The selection seems to capture different aspects related to the design and usage of products.\n",
        "\n",
        "- Words: The top 5 words identified by this method are \"keyboard,\" \"word,\" \"product,\" \"even,\" and \"new.\" These words reflect the importance of keyboards, the general concept of words, and the significance of products in the context of the extracted sentences.\n",
        "\n",
        "There is no surprise that extracted keywords are emerging in extracted sentences as well, because in our method, we assumed that important sentences cotain important words and vice versa.\n",
        "\n",
        "Method-2:\n",
        "\n",
        "- Sentences: The selected sentences from this method cover topics such as the impact of new technologies, the shift from traditional writing to new media, listening speed, and the need for a new term to describe devices that combine multiple functions. These sentences provide insights into the changing landscape of technology and communication.\n",
        "\n",
        "- Words: The top 5 words identified by this method are \"product,\" \"new,\" \"feature,\" \"company,\" and \"competition.\" These words emphasize the importance of products, the introduction of new features, the role of companies, and the competitive environment.\n",
        "\n",
        "Extracted sentences from this method look much less similar to each other, and it was predictable because we were trying to decrease the similarity of top-rank sentences reported by this method.\n",
        "\n",
        "In general, important words & sentences reported from the 1st method are strongly connected, and important sentences of the 2nd method look more unique and less similar to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymM3bE0fVQS9"
      },
      "source": [
        "\n",
        "Thanks for your valuable time and attention! This notebook is available in the link belowüòÄ\n",
        "\n",
        "https://drive.google.com/file/d/12eBTqHXboWXehfCPu4nmoDLTxnpC5UZl/view?usp=sharing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
